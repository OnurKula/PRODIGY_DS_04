{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4dde1648-ae6d-4167-a197-5ed469979294",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d126199f-e69a-4bce-995a-0573b1c73dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\16088\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\16088\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ef89794-cec8-4822-8e95-6fce889d3f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = [\"ID\", \"Category\", \"Sentiment\", \"Text\"]\n",
    "data = pd.read_csv(\"twitter_training.csv\",names=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab4731c1-19ca-4d58-81bf-0973814eb1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"\\nMissing values in the dataset:\\n\", data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69b54c61-6aba-4f09-81b1-8f4ced8aa2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values in 'Text' column (Option 1: Fill with empty string)\n",
    "#data['Text'].fillna(\"\", inplace=True)\n",
    "# Remove rows with missing values in the 'Text' column\n",
    "#data = data.dropna(subset=['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dba10f42-6fe3-476a-9ad1-b30d80ddae9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Category</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands and i will murder yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>I am coming to the borders and I will kill you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands and i will kill you ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im coming on borderlands and i will murder you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands 2 and i will murder ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ID     Category Sentiment  \\\n",
       "0  2401  Borderlands  Positive   \n",
       "1  2401  Borderlands  Positive   \n",
       "2  2401  Borderlands  Positive   \n",
       "3  2401  Borderlands  Positive   \n",
       "4  2401  Borderlands  Positive   \n",
       "\n",
       "                                                Text  \n",
       "0  im getting on borderlands and i will murder yo...  \n",
       "1  I am coming to the borders and I will kill you...  \n",
       "2  im getting on borderlands and i will kill you ...  \n",
       "3  im coming on borderlands and i will murder you...  \n",
       "4  im getting on borderlands 2 and i will murder ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c40ae32-4a8c-4b11-bed5-efc41e7d847b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"\\nMissing values in the dataset:\\n\", data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8208976b-09d1-4cfb-97d6-1dfabfd2a237",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 74682 entries, 0 to 74681\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   ID         74682 non-null  int64 \n",
      " 1   Category   74682 non-null  object\n",
      " 2   Sentiment  74682 non-null  object\n",
      " 3   Text       73996 non-null  object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 2.3+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1e44b4a-3b31-4c19-8533-57f262c6298a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID            int64\n",
       "Category     object\n",
       "Sentiment    object\n",
       "Text         object\n",
       "dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3fcd827-1bd9-4eda-a5d6-43c3853eadf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Text  \\\n",
      "0  im getting on borderlands and i will murder yo...   \n",
      "1  I am coming to the borders and I will kill you...   \n",
      "2  im getting on borderlands and i will kill you ...   \n",
      "3  im coming on borderlands and i will murder you...   \n",
      "4  im getting on borderlands 2 and i will murder ...   \n",
      "\n",
      "                      Cleaned_Text  \n",
      "0    im getting borderlands murder  \n",
      "1              coming borders kill  \n",
      "2      im getting borderlands kill  \n",
      "3     im coming borderlands murder  \n",
      "4  im getting borderlands 2 murder  \n"
     ]
    }
   ],
   "source": [
    "# Define a function to clean the text\n",
    "def clean_text(text):\n",
    "    # Ensure the input is a string\n",
    "    text = str(text).lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # Remove user mentions and hashtags\n",
    "    text = re.sub(r'\\@\\w+|\\#', '', text)\n",
    "    \n",
    "    # Remove non-alphanumeric characters except spaces\n",
    "    text = re.sub(r'[^A-Za-z0-9 ]+', ' ', text)\n",
    "    \n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stop words\n",
    "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
    "    \n",
    "    # Join tokens back to a single string\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply the cleaning function to the 'Text' column\n",
    "data['Cleaned_Text'] = data['Text'].apply(clean_text)\n",
    "\n",
    "# Display cleaned data\n",
    "print(data[['Text', 'Cleaned_Text']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07ab91a6-4da6-49e3-b47b-280ed4e354f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(74682, 5)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "890e22b1-facc-4a8b-8329-87299751c145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Text  \\\n",
      "0  im getting on borderlands and i will murder yo...   \n",
      "1  I am coming to the borders and I will kill you...   \n",
      "2  im getting on borderlands and i will kill you ...   \n",
      "3  im coming on borderlands and i will murder you...   \n",
      "4  im getting on borderlands 2 and i will murder ...   \n",
      "\n",
      "                      Cleaned_Text Sentiment  Encoded_Sentiment  \n",
      "0    im getting borderlands murder  Positive                  3  \n",
      "1              coming borders kill  Positive                  3  \n",
      "2      im getting borderlands kill  Positive                  3  \n",
      "3     im coming borderlands murder  Positive                  3  \n",
      "4  im getting borderlands 2 murder  Positive                  3  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Encode target labels\n",
    "le = LabelEncoder()\n",
    "data['Encoded_Sentiment'] = le.fit_transform(data['Sentiment'])\n",
    "\n",
    "# Display cleaned data\n",
    "print(data[['Text', 'Cleaned_Text', 'Sentiment', 'Encoded_Sentiment']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "88b241da-3289-48d5-9415-c7f2687911fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Category</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Text</th>\n",
       "      <th>Cleaned_Text</th>\n",
       "      <th>Encoded_Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands and i will murder yo...</td>\n",
       "      <td>im getting borderlands murder</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>I am coming to the borders and I will kill you...</td>\n",
       "      <td>coming borders kill</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands and i will kill you ...</td>\n",
       "      <td>im getting borderlands kill</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im coming on borderlands and i will murder you...</td>\n",
       "      <td>im coming borderlands murder</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands 2 and i will murder ...</td>\n",
       "      <td>im getting borderlands 2 murder</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ID     Category Sentiment  \\\n",
       "0  2401  Borderlands  Positive   \n",
       "1  2401  Borderlands  Positive   \n",
       "2  2401  Borderlands  Positive   \n",
       "3  2401  Borderlands  Positive   \n",
       "4  2401  Borderlands  Positive   \n",
       "\n",
       "                                                Text  \\\n",
       "0  im getting on borderlands and i will murder yo...   \n",
       "1  I am coming to the borders and I will kill you...   \n",
       "2  im getting on borderlands and i will kill you ...   \n",
       "3  im coming on borderlands and i will murder you...   \n",
       "4  im getting on borderlands 2 and i will murder ...   \n",
       "\n",
       "                      Cleaned_Text  Encoded_Sentiment  \n",
       "0    im getting borderlands murder                  3  \n",
       "1              coming borders kill                  3  \n",
       "2      im getting borderlands kill                  3  \n",
       "3     im coming borderlands murder                  3  \n",
       "4  im getting borderlands 2 murder                  3  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f62c8ace-77a3-4bf3-9722-076202b433c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[\"Cleaned_Text\"]\n",
    "y = data[\"Encoded_Sentiment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8371f1c6-39a6-40fc-8ebe-1b429784c600",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33, random_state=53\n",
    ")\n",
    "\n",
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "count_train = count_vectorizer.fit_transform(X_train.values)\n",
    "count_test = count_vectorizer.transform(X_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "36acb15b-e74c-4672-975e-ddb23aa715b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (50036,)\n",
      "Testing set shape: (24646,)\n"
     ]
    }
   ],
   "source": [
    "# Display shapes of train and test sets to confirm\n",
    "print(\"Training set shape:\", X_train.shape)\n",
    "print(\"Testing set shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bd114909-33da-4437-9aab-6ec9a46a250c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score :  77.58256917958289\n",
      "Accuracy Score :  75.22518867158972\n",
      "Accuracy Score :  73.33441532094457\n",
      "Accuracy Score :  71.81287024263571\n",
      "Accuracy Score :  70.57128945873569\n"
     ]
    }
   ],
   "source": [
    "alpha =  [0.1, 0.5, 1.0, 1.5, 2.0]\n",
    "for i in alpha:\n",
    "    \n",
    "    nb_classifier = MultinomialNB(alpha=i)\n",
    "    \n",
    "    nb_classifier.fit(count_train, y_train)\n",
    "    pred = nb_classifier.predict(count_test)\n",
    "    print(\"Accuracy Score : \",metrics.accuracy_score(y_test, pred)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c06fd0d3-2a47-4724-a7dd-354c5041b7d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16088\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\16088\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 80.90968108415159\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "  Irrelevant       0.81      0.75      0.78      4270\n",
      "    Negative       0.84      0.83      0.84      7400\n",
      "     Neutral       0.83      0.78      0.80      6077\n",
      "    Positive       0.77      0.85      0.81      6899\n",
      "\n",
      "    accuracy                           0.81     24646\n",
      "   macro avg       0.81      0.80      0.81     24646\n",
      "weighted avg       0.81      0.81      0.81     24646\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16088\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\16088\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\16088\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\16088\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\16088\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 88.55392355757526\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "  Irrelevant       0.92      0.84      0.88      4270\n",
      "    Negative       0.91      0.90      0.90      7400\n",
      "     Neutral       0.90      0.87      0.88      6077\n",
      "    Positive       0.84      0.92      0.88      6899\n",
      "\n",
      "    accuracy                           0.89     24646\n",
      "   macro avg       0.89      0.88      0.88     24646\n",
      "weighted avg       0.89      0.89      0.89     24646\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16088\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\16088\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\16088\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 88.47277448673213\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "  Irrelevant       0.92      0.82      0.87      4270\n",
      "    Negative       0.90      0.90      0.90      7400\n",
      "     Neutral       0.90      0.87      0.88      6077\n",
      "    Positive       0.83      0.92      0.87      6899\n",
      "\n",
      "    accuracy                           0.88     24646\n",
      "   macro avg       0.89      0.88      0.88     24646\n",
      "weighted avg       0.89      0.88      0.88     24646\n",
      "\n",
      "Accuracy: 87.60042197516839\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "  Irrelevant       0.92      0.80      0.86      4270\n",
      "    Negative       0.89      0.90      0.90      7400\n",
      "     Neutral       0.89      0.86      0.87      6077\n",
      "    Positive       0.83      0.91      0.87      6899\n",
      "\n",
      "    accuracy                           0.88     24646\n",
      "   macro avg       0.88      0.87      0.87     24646\n",
      "weighted avg       0.88      0.88      0.88     24646\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16088\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\16088\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 88.47277448673213\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "  Irrelevant       0.97      0.83      0.90      4270\n",
      "    Negative       0.96      0.86      0.90      7400\n",
      "     Neutral       0.95      0.87      0.91      6077\n",
      "    Positive       0.75      0.96      0.85      6899\n",
      "\n",
      "    accuracy                           0.88     24646\n",
      "   macro avg       0.91      0.88      0.89     24646\n",
      "weighted avg       0.90      0.88      0.89     24646\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16088\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\16088\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 88.29830398441938\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "  Irrelevant       0.98      0.82      0.89      4270\n",
      "    Negative       0.96      0.86      0.91      7400\n",
      "     Neutral       0.96      0.86      0.91      6077\n",
      "    Positive       0.74      0.97      0.84      6899\n",
      "\n",
      "    accuracy                           0.88     24646\n",
      "   macro avg       0.91      0.88      0.89     24646\n",
      "weighted avg       0.90      0.88      0.89     24646\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16088\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\16088\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 87.79923719873408\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "  Irrelevant       0.98      0.81      0.88      4270\n",
      "    Negative       0.96      0.86      0.90      7400\n",
      "     Neutral       0.96      0.85      0.90      6077\n",
      "    Positive       0.73      0.97      0.84      6899\n",
      "\n",
      "    accuracy                           0.88     24646\n",
      "   macro avg       0.91      0.87      0.88     24646\n",
      "weighted avg       0.90      0.88      0.88     24646\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16088\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\16088\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 82.63004138602614\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "  Irrelevant       0.99      0.74      0.85      4270\n",
      "    Negative       0.98      0.75      0.85      7400\n",
      "     Neutral       0.96      0.79      0.87      6077\n",
      "    Positive       0.63      0.99      0.77      6899\n",
      "\n",
      "    accuracy                           0.83     24646\n",
      "   macro avg       0.89      0.82      0.84     24646\n",
      "weighted avg       0.88      0.83      0.83     24646\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16088\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\16088\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\16088\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\16088\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\16088\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 81.31948389190944\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "  Irrelevant       1.00      0.72      0.83      4270\n",
      "    Negative       0.98      0.75      0.85      7400\n",
      "     Neutral       0.96      0.76      0.85      6077\n",
      "    Positive       0.61      0.99      0.75      6899\n",
      "\n",
      "    accuracy                           0.81     24646\n",
      "   macro avg       0.89      0.80      0.82     24646\n",
      "weighted avg       0.88      0.81      0.82     24646\n",
      "\n",
      "Accuracy: 74.15402093646027\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "  Irrelevant       1.00      0.62      0.77      4270\n",
      "    Negative       0.99      0.63      0.77      7400\n",
      "     Neutral       0.97      0.67      0.79      6077\n",
      "    Positive       0.52      0.99      0.69      6899\n",
      "\n",
      "    accuracy                           0.74     24646\n",
      "   macro avg       0.87      0.73      0.76     24646\n",
      "weighted avg       0.86      0.74      0.75     24646\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Vectorize the text data\n",
    "for i in range(1,5):\n",
    "    \n",
    "    for x in range(1,5):\n",
    "        if x>=i:\n",
    "            \n",
    "            bow_counts = CountVectorizer(tokenizer=word_tokenize, stop_words=stop_words, ngram_range=(i, x))\n",
    "            X_train_bow = bow_counts.fit_transform(X_train)\n",
    "            X_test_bow = bow_counts.transform(X_test)\n",
    "            \n",
    "            # Train Logistic Regression model\n",
    "            model = LogisticRegression(C=1, solver=\"liblinear\", max_iter=200, class_weight='balanced')\n",
    "            model.fit(X_train_bow, y_train)\n",
    "            \n",
    "            # Make predictions\n",
    "            y_pred = model.predict(X_test_bow)\n",
    "            \n",
    "            # Evaluate the model\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            print(\"Accuracy:\", accuracy * 100)\n",
    "            print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred, target_names=le.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7452097b-ce75-4429-ba4c-173f0c568935",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16088\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\16088\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 88.55392355757526\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "  Irrelevant       0.92      0.84      0.88      4270\n",
      "    Negative       0.91      0.90      0.90      7400\n",
      "     Neutral       0.90      0.87      0.88      6077\n",
      "    Positive       0.84      0.92      0.88      6899\n",
      "\n",
      "    accuracy                           0.89     24646\n",
      "   macro avg       0.89      0.88      0.88     24646\n",
      "weighted avg       0.89      0.89      0.89     24646\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16088\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "bow_counts = CountVectorizer(tokenizer=word_tokenize, stop_words=stop_words, ngram_range=(1, 2))\n",
    "X_train_bow = bow_counts.fit_transform(X_train)\n",
    "X_test_bow = bow_counts.transform(X_test)\n",
    "            \n",
    "# Train Logistic Regression model\n",
    "model = LogisticRegression(C=1, solver=\"liblinear\", max_iter=200, class_weight='balanced')\n",
    "model.fit(X_train_bow, y_train)\n",
    "            \n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test_bow)\n",
    "            \n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy * 100)\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred, target_names=le.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "352eccc1-bd48-47f5-9907-f7718135eefd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model Accuracy: 77.86659092753389\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "  Irrelevant       0.73      0.72      0.73      4270\n",
      "    Negative       0.84      0.80      0.82      7400\n",
      "     Neutral       0.77      0.77      0.77      6077\n",
      "    Positive       0.76      0.81      0.78      6899\n",
      "\n",
      "    accuracy                           0.78     24646\n",
      "   macro avg       0.77      0.77      0.77     24646\n",
      "weighted avg       0.78      0.78      0.78     24646\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=10000, stop_words=stop_words)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Set up parameter grid for Logistic Regression\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'solver': ['liblinear', 'lbfgs'],\n",
    "    'max_iter': [200, 500]\n",
    "}\n",
    "\n",
    "# Initialize the Logistic Regression model\n",
    "logistic_model = LogisticRegression(class_weight='balanced')\n",
    "\n",
    "# Perform GridSearchCV for hyperparameter tuning\n",
    "grid_search = GridSearchCV(logistic_model, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Best model from Grid Search\n",
    "best_logistic_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions with the best model\n",
    "y_pred = best_logistic_model.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate the best model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Best Model Accuracy:\", accuracy * 100)\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred, target_names=le.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e318a18-2681-4d66-865a-d83f8c299752",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e1d2e9-1fe2-4b96-ab0b-37fd37b8414c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd15616-c4d3-4085-abbc-bca14497f598",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
